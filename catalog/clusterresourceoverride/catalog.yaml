---
defaultChannel: stable
name: clusterresourceoverride
schema: olm.package
---
entries:
- name: clusterresourceoverride-operator.v4.17.0-202409100034
  skipRange: '>=4.3.0 <4.17.0-202409100034'
- name: clusterresourceoverride-operator.v4.17.0-202410011205
  skipRange: '>=4.3.0 <4.17.0-202410011205'
- name: clusterresourceoverride-operator.v4.17.0-202410112132
  skipRange: '>=4.3.0 <4.17.0-202410112132'
- name: clusterresourceoverride-operator.v4.17.0-202410241236
  skipRange: '>=4.3.0 <4.17.0-202410241236'
- name: clusterresourceoverride-operator.v4.17.0-202411080405
  skipRange: '>=4.3.0 <4.17.0-202411080405'
- name: clusterresourceoverride-operator.v4.17.0-202411190204
  skipRange: '>=4.3.0 <4.17.0-202411190204'
- name: clusterresourceoverride-operator.v4.17.0-202411251634
  skipRange: '>=4.3.0 <4.17.0-202411251634'
- name: clusterresourceoverride-operator.v4.17.0-202412021735
  skipRange: '>=4.3.0 <4.17.0-202412021735'
- name: clusterresourceoverride-operator.v4.17.0-202412050933
  skipRange: '>=4.3.0 <4.17.0-202412050933'
- name: clusterresourceoverride-operator.v4.17.0-202412170235
  skipRange: '>=4.3.0 <4.17.0-202412170235'
  skips:
  - clusterresourceoverride-operator.v4.17.0-202408151545
  - clusterresourceoverride-operator.v4.17.0-202412122104
  - clusterresourceoverride-operator.v4.17.0-202409030638
  - clusterresourceoverride-operator.v4.17.0-202409100034
  - clusterresourceoverride-operator.v4.17.0-202407302009
  - clusterresourceoverride-operator.v4.17.0-202412021735
  - clusterresourceoverride-operator.v4.17.0-202411230231
  - clusterresourceoverride-operator.v4.17.0-202408260836
  - clusterresourceoverride-operator.v4.17.0-202411080405
  - clusterresourceoverride-operator.v4.17.0-202410241236
  - clusterresourceoverride-operator.v4.17.0-202412050933
  - clusterresourceoverride-operator.v4.17.0-202405291241
  - clusterresourceoverride-operator.v4.17.0-202411190204
  - clusterresourceoverride-operator.v4.17.0-202410112132
  - clusterresourceoverride-operator.v4.17.0-202410011205
  - clusterresourceoverride-operator.v4.17.0-202406202314
  - clusterresourceoverride-operator.v4.17.0-202411251634
  - clusterresourceoverride-operator.v4.17.0-202411131633
  - clusterresourceoverride-operator.v4.17.0-202407090112
name: stable
package: clusterresourceoverride
schema: olm.channel
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:69914ab42c920f527c4e876258acc4e9bf3c5673f5a80823dc4b2e771420716c
name: clusterresourceoverride-operator.v4.17.0-202409100034
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202409100034
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:7fd713403455782d8988831ffddafaed633a2487100c0e395828e79ae4cb9101
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202409100034'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:69914ab42c920f527c4e876258acc4e9bf3c5673f5a80823dc4b2e771420716c
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:7fd713403455782d8988831ffddafaed633a2487100c0e395828e79ae4cb9101
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:2b46c257720a94fda23ce19330b29b5d0972880052acfdc819f5f15c000fda90
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:46a99e1821f4f960ed8799bd1ddd3a735e9684b7efc52e866940f52297afbf6a
name: clusterresourceoverride-operator.v4.17.0-202410011205
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202410011205
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:8137b0065f51d70bcf41ad2c7d251982fb6f06aff6527447161faf38cbe7d53e
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202410011205'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:46a99e1821f4f960ed8799bd1ddd3a735e9684b7efc52e866940f52297afbf6a
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:8137b0065f51d70bcf41ad2c7d251982fb6f06aff6527447161faf38cbe7d53e
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:c43a8e075125e1beb70608c244775a64851f9636d036f9fe624eb5587554ecdb
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:83e0ee9e3e04d7b40476e9d1b8496422d39322449fa613a447e8a209986283c3
name: clusterresourceoverride-operator.v4.17.0-202410112132
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202410112132
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:5d0e85fd795ac8d510467d664c4943beb89f8df55a4d93fd45545560b340d0c3
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202410112132'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:83e0ee9e3e04d7b40476e9d1b8496422d39322449fa613a447e8a209986283c3
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:5d0e85fd795ac8d510467d664c4943beb89f8df55a4d93fd45545560b340d0c3
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:612d1420e51cd5999157c23af13c5f127db0830ba3997f8a0c83a214a8419172
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:0f4ecdd26824f2336a227e168f0270684a319236f280a2b2c7da37091a7ce80b
name: clusterresourceoverride-operator.v4.17.0-202410241236
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202410241236
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:fb227ea1e738719186419f94f98d84ac443852bfee5fbcae53308b73777c2c03
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202410241236'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:0f4ecdd26824f2336a227e168f0270684a319236f280a2b2c7da37091a7ce80b
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:fb227ea1e738719186419f94f98d84ac443852bfee5fbcae53308b73777c2c03
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:f258f21b288b0b17584b0d1cd3387ebe46dd4098c7fb14c9bc7c8915d5ecc4c2
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:78a1703fdf2168807c635ff309923152d9b35188f60041eb7f390d00c01dd665
name: clusterresourceoverride-operator.v4.17.0-202411080405
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202411080405
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:e3ff3d5d503b7a6074dd314a16cfc68b0a0f5597a5d75d63691c5c8da723beeb
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202411080405'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:78a1703fdf2168807c635ff309923152d9b35188f60041eb7f390d00c01dd665
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:e3ff3d5d503b7a6074dd314a16cfc68b0a0f5597a5d75d63691c5c8da723beeb
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:f258f21b288b0b17584b0d1cd3387ebe46dd4098c7fb14c9bc7c8915d5ecc4c2
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:4471a959199112e777513981c38ba8931d6fb494b971051b8a94121a482735f4
name: clusterresourceoverride-operator.v4.17.0-202411190204
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202411190204
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:ca79c7c8ca009e45c46ae6e0011805630deb809c5353f4ee0b8b055f02ae6c05
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202411190204'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:4471a959199112e777513981c38ba8931d6fb494b971051b8a94121a482735f4
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:ca79c7c8ca009e45c46ae6e0011805630deb809c5353f4ee0b8b055f02ae6c05
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:8fdd7f9099cb288391a5078872873052e993ade17db8520028db6c87cea2ecaf
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:1d5e988b70e304e0fa8f88a461042b1cbe69d786d2b595e5ff8c880d72411645
name: clusterresourceoverride-operator.v4.17.0-202411251634
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202411251634
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:764e2926db7bc832769652dd9f8333dee500720c2d8de44790155f21d159614d
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202411251634'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:1d5e988b70e304e0fa8f88a461042b1cbe69d786d2b595e5ff8c880d72411645
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:764e2926db7bc832769652dd9f8333dee500720c2d8de44790155f21d159614d
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:0f0a073d425a47c25832bf9f4cc15e5ceffbbf58260fc48423074f3d80fb9c51
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:fcf72c2b0c7e9c49016260011cf63f80e23e5d237ca0621e05016cbad7d77ae5
name: clusterresourceoverride-operator.v4.17.0-202412021735
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202412021735
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:c7d6d32a886bb77391a2c58502b79b382a4871889fd97afd9eb2a2c4baebebb0
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202412021735'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:fcf72c2b0c7e9c49016260011cf63f80e23e5d237ca0621e05016cbad7d77ae5
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:c7d6d32a886bb77391a2c58502b79b382a4871889fd97afd9eb2a2c4baebebb0
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:a8515345ecd259dcf204f79343aed34d85e4f842c9ad6fa6dec405fb5f5e0bbd
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:5ac9480425bf0f118d9bcdc9a3669ef744ecb413e56afd65ea14688caa060be0
name: clusterresourceoverride-operator.v4.17.0-202412050933
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202412050933
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:071b17cfc2cd8b7456c5610f3a372563e2f2ea17dceca320da9516a58025c6c6
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202412050933'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:5ac9480425bf0f118d9bcdc9a3669ef744ecb413e56afd65ea14688caa060be0
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:071b17cfc2cd8b7456c5610f3a372563e2f2ea17dceca320da9516a58025c6c6
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:c8469ca83b738bc4db8b68bb6ca9333803392f222e67ed407ce5de197f8078c5
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
---
image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:74b050fa74311c29413212b1c4dd6f3a5a9f6c451f143d3d21b323f34c8c15e1
name: clusterresourceoverride-operator.v4.17.0-202412170235
package: clusterresourceoverride
properties:
- type: olm.gvk
  value:
    group: operator.autoscaling.openshift.io
    kind: ClusterResourceOverride
    version: v1
- type: olm.package
  value:
    packageName: clusterresourceoverride
    version: 4.17.0-202412170235
- type: olm.csv.metadata
  value:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "operator.autoscaling.openshift.io/v1",
            "kind": "ClusterResourceOverride",
            "metadata": {
              "name": "cluster"
            },
            "spec": {
              "podResourceOverride": {
                "spec": {
                  "memoryRequestToLimitPercent": 50,
                  "cpuRequestToLimitPercent": 25,
                  "limitCPUToMemoryPercent": 200
                }
              }
            }
          }
        ]
      capabilities: Seamless Upgrades
      categories: OpenShift Optional
      certifiedLevel: Primed
      containerImage: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:58c848d1bfc059194150f950b5cd83a576a4c9ebf549346b1f95fd8adcc71708
      createdAt: 2019/11/15
      description: An operator to manage the OpenShift ClusterResourceOverride Mutating
        Admission Webhook Server
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "true"
      features.operators.openshift.io/proxy-aware: "false"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      healthIndex: B
      olm.skipRange: '>=4.3.0 <4.17.0-202412170235'
      operators.openshift.io/valid-subscription: '["OpenShift Kubernetes Engine",
        "OpenShift Container Platform", "OpenShift Platform Plus"]'
      repository: https://github.com/openshift/cluster-resource-override-admission-operator
      support: Red Hat
    apiServiceDefinitions: {}
    crdDescriptions:
      owned:
      - description: Represents an instance of ClusterResourceOverride Admission Webhook
        displayName: ClusterResourceOverride
        kind: ClusterResourceOverride
        name: clusterresourceoverrides.operator.autoscaling.openshift.io
        version: v1
    description: "ClusterResourceOverride\n==============\n\nContainers can specify
      compute resource requests and limits. Requests are used for scheduling your
      container and provide a minimum service guarantee. Limits constrain the amount
      of compute resource that may be consumed on your node.\n\nThe scheduler attempts
      to optimize the compute resource use across all nodes in your cluster. It places
      pods onto specific nodes, taking the pods' compute resource requests and nodes'
      available capacity into consideration.\n\nRequests and limits enable administrators
      to allow and manage the overcommitment of resources on a node, which may be
      desirable in development environments where a trade off of guaranteed performance
      for capacity is acceptable.\n\n### Requests and Limits\n\nFor each compute resource,
      a container may specify a resource request and limit. Scheduling decisions are
      made based on the request to ensure that a node has enough capacity available
      to meet the requested value. If a container specifies limits, but omits requests,
      the requests are defaulted to the limits. A container is not able to exceed
      the specified limit on the node.\n\nThe enforcement of limits is dependent upon
      the compute resource type. If a container makes no request or limit, the container
      is scheduled to a node with no resource guarantees. In practice, the container
      is able to consume as much of the specified resource as is available with the
      lowest local priority. In low resource situations, containers that specify no
      resource requests are given the lowest quality of service.\n\n### Compute Resources\n\nThe
      node-enforced behavior for compute resources is specific to the resource type.\n\n####
      CPU\n\nA container is guaranteed the amount of CPU it requests and is additionally
      able to consume excess CPU available on the node, up to any limit specified
      by the container. If multiple containers are attempting to use excess CPU, CPU
      time is distributed based on the amount of CPU requested by each container.\n\nFor
      example, if one container requested 500m of CPU time and another container requested
      250m of CPU time, then any extra CPU time available on the node is distributed
      among the containers in a 2:1 ratio. If a container specified a limit, it will
      be throttled not to use more CPU than the specified limit.\n\nCPU requests are
      enforced using the CFS shares support in the Linux kernel. By default, CPU limits
      are enforced using the CFS quota support in the Linux kernel over a 100ms measuring
      interval, though this can be disabled.\n\n#### Memory\n\nA container is guaranteed
      the amount of memory it requests. A container may use more memory than requested,
      but once it exceeds its requested amount, it could be killed in a low memory
      situation on the node.\n\nIf a container uses less memory than requested, it
      will not be killed unless system tasks or daemons need more memory than was
      accounted for in the nodeâ€™s resource reservation. If a container specifies a
      limit on memory, it is immediately killed if it exceeds the limit amount.\n\n###
      Configuring the Cluster for Overcommitment\n\nScheduling is based on resources
      requested, while quota and hard limits refer to resource limits, which can be
      set higher than requested resources. The difference between request and limit
      determines the level of overcommit; for instance, if a container is given a
      memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the
      1Gi request being available on the node, but could use up to 2Gi; so it is 200%
      overcommitted. If OpenShift Container Platform administrators would like to
      control the level of overcommit and manage container density on nodes, ClusterResourceOverride
      Admission Webhook can be configured to override the ratio between request and
      limit set on developer containers. In conjunction with a per-project LimitRange
      specifying limits and defaults, this adjusts the container limit and request
      to achieve the desired level of overcommit.\n\nThis requires creating a custom
      resource of `ClusterResourceOverride` type as in the following example:\n\n
      \   \n    - apiVersion: operator.autoscaling.openshift.io/v1\n    - kind: ClusterResourceOverride\n
      \   - metadata:\n    -   name: cluster\n    - spec:\n    -   podResourceOverride:\n
      \   -     spec:\n    -       memoryRequestToLimitPercent: 25\n    -       cpuRequestToLimitPercent:
      25\n    -       limitCPUToMemoryPercent: 200\n    \n **memoryRequestToLimitPercent**:
      (optional, 1-100) If a container memory limit has been specified or defaulted,
      the memory request is overridden to this percentage of the limit.\n\n **cpuRequestToLimitPercent**:
      (optional, 1-100) If a container CPU limit has been specified or defaulted,
      the CPU request is overridden to this percentage of the limit.\n\n **limitCPUToMemoryPercent**:
      (optional, positive integer) If a container memory limit has been specified
      or defaulted, the CPU limit is overridden to a percentage of the memory limit,
      with a 100 percentage scaling 1Gi of RAM to equal 1 CPU core. This is processed
      prior to overriding CPU request (if configured).\n\n Note that these overrides
      have no effect if no limits have been set on containers. [Create a LimitRange
      object] (https://docs.openshift.com/container-platform/3.3/admin_guide/limits.html#admin-guide-limits)
      with default limits (per individual project, or in the [project template](https://docs.openshift.com/container-platform/3.3/admin_guide/managing_projects.html#modifying-the-template-for-new-projects))
      in order to ensure that the overrides apply.\n\nWhen configured, overrides can
      be enabled per-project by applying the following label.\n    ```\n    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled:
      \"true\"\n    ```\n\n"
    displayName: ClusterResourceOverride Operator
    installModes:
    - supported: true
      type: OwnNamespace
    - supported: true
      type: SingleNamespace
    - supported: false
      type: MultiNamespace
    - supported: false
      type: AllNamespaces
    keywords:
    - deschedule
    - scale
    - binpack
    - efficiency
    labels:
      operatorframework.io/arch.amd64: supported
      operatorframework.io/arch.ppc64le: supported
      operatorframework.io/arch.s390x: supported
    maintainers:
    - email: support@redhat.com
      name: Red Hat
    provider:
      name: Red Hat
relatedImages:
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-operator-bundle@sha256:74b050fa74311c29413212b1c4dd6f3a5a9f6c451f143d3d21b323f34c8c15e1
  name: ""
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9-operator@sha256:58c848d1bfc059194150f950b5cd83a576a4c9ebf549346b1f95fd8adcc71708
  name: ose-clusterresourceoverride-rhel9-operator
- image: registry.redhat.io/openshift4/ose-clusterresourceoverride-rhel9@sha256:b11c7317edc4ace60101bacce04dc6c9e5201ad48f3fe99d66be8cd4770c882e
  name: ose-clusterresourceoverride-rhel9
schema: olm.bundle
